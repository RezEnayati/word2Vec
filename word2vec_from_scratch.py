# -*- coding: utf-8 -*-
"""word2vec_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLKKGTIDQ_AIw8tRlW4w9PzYXlXi9H5p
"""

import numpy as np
import collections
import random
import math
import re

corpus = """
 computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs, which enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster.

A broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users.

Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved."""

def tokenize(corpus):
  sentences = re.split(r'[.!?]', corpus) # seperate into sentences
  sentences = [s.strip() for s in sentences if s.strip() != ""] #removes white space
  sentences = [re.sub(r'[^a-zA-Z\s\']', "", s ) for s in sentences] #remove the puncations
  tokens = [s.lower().split() for s in sentences] # tokenize
  return tokens

def unique_vocab(tokens):
  return list(set([word for token in tokens for word in token])) #flatten and create set to get unique words

def vocabulary_builder(tokens):
  unique = unique_vocab(tokens)
  word2idx = {word:i for i,word in enumerate(unique)} # word to index dictionary
  idx2word = {i:word for i,word in enumerate(unique)} # index to word dictionary
  return word2idx, idx2word

def get_word_counts(tokens): # gets the frequency of words
  frequency = {}
  for sentences in tokens:
    for word in sentences:
      frequency[word] = frequency.get(word, 0) + 1
  return frequency

def create_skip_gram_pairs(tokens, window_size = 3): # create the training pairs with a default window size of 5
  pairs = []
  for sentence in tokens:
    sentence_len = len(sentence)
    for i, center_word in enumerate(sentence):
      for j in range(max(0, i - window_size), min(i + window_size + 1, sentence_len)):
        if i != j:
          pairs.append((center_word, sentence[j]))
  return pairs

def build_unigram_distribution(word2idx, word_counts):
  idx2prob = {}
  word_counts = {key: value**0.75 for key, value in word_counts.items()}
  total = sum(word_counts.values())
  unigram = {word2idx[key]: value / total for key, value in word_counts.items()}

  return unigram

def sample_negatives(word2idx,unigram_dic,pair, k=5):
  # since pair is a pair of words we have to turn them into the indicies
  exclude_idx = [word2idx[word] for word in pair]
  keys = []
  probs = []
  for key,value in unigram_dic.items():
    if key not in exclude_idx:
      keys.append(key)
      probs.append(value)

  # now we have to normalize again because now it doesn't add up to zero
  total = np.sum(probs)
  probs = [prob / total for prob in probs]

  negative_samples = []

  for i in range(k):
    rand = np.random.choice(np.arange(0, len(keys)), p=probs) #this is how we can get random numbers using a custom distribution
    negative_samples.append(keys[rand])

  return negative_samples

#so this is where we create the matrcies U and V for the word embeddings
# U corresponds to when the word is in the context
# V corresponds to when the word is in the center
# The order should be the same as the one in the word2idx, order
def initialize_embeddings(vocab_size, embedding_dim):
  low = (-1) / np.sqrt(embedding_dim)
  high = 1 / np.sqrt(embedding_dim)

  U = np.random.uniform(low, high, size = (vocab_size, embedding_dim)) #context
  V = np.random.uniform(low, high, size = (vocab_size, embedding_dim)) #center

  return V, U

def sigmoid(x): #simple sigmoid helper
  return 1 / (1 + np.exp(-x))

def compute_loss(v_c, u_o, u_k):
  sum = 0
  for u_neg in u_k:
    product = np.dot(v_c, u_neg)
    sig = sigmoid(-product)
    sum +=  np.log(sig)

  dot_prod = np.dot(u_o, v_c)
  sigmoid_of_prod = sigmoid(dot_prod)
  log_of_sig = np.log(sigmoid_of_prod)
  return (-log_of_sig - sum)

def compute_gradients(v_c, u_o, u_k):
    # Gradient w.r.t v_c
    sum_v_c = np.sum([sigmoid(np.dot(v_c, u_neg)) * u_neg for u_neg in u_k], axis=0)
    d_v_c = (sigmoid(np.dot(v_c, u_o)) - 1) * u_o + sum_v_c

    # Gradient w.r.t u_o
    d_u_o = (sigmoid(np.dot(u_o, v_c)) - 1) * v_c

    # Gradient w.r.t each u_k
    d_u_k = [sigmoid(np.dot(u_neg, v_c)) * v_c for u_neg in u_k]

    return d_v_c, d_u_o, d_u_k

#function for the training loop
"""
we first take in a pair from the skip-gram pairs, a learning rate, matrix U an V along with the word2idx dictionary
then we get the get the negative pairs, compute the dot product of the center vector and the context vector
get all the vectors for the negative samples.
compute the dot product of the venter vector and the outside vector
compute the dot for the negative samples with the center vector
get a positive score using sigmoid for the dot_product of the center and the real context
also the sigmoid of the negative samples and the center words
comput the loss
compute the gradients by computing the paritial derivative of loss w.r.t v_c, u_o and u_k
then we multiply these gradients by the learning rate and update U and V
and return the loss
"""

def training_step(pair, learning_rate, V, U, word2idx, unigram_dic):
  negative_samples = sample_negatives(word2idx, unigram_dic, pair) #these are the negative samples give us indicies
  center_idx = word2idx[pair[0]]
  outside_idx = word2idx[pair[1]]


  v_c = V[center_idx].copy()  #vector for the center word
  u_o = U[outside_idx].copy() #vector for the outside word
  u_k = [U[idx].copy() for idx in negative_samples]

  loss = compute_loss(v_c, u_o, u_k)

  d_v_c, d_u_o, d_u_k = compute_gradients(v_c, u_o, u_k)

  V[center_idx] -= learning_rate * d_v_c
  U[outside_idx] -= learning_rate * d_u_o

  for i,idx in enumerate(negative_samples):
    U[idx] -= learning_rate * d_u_k[i]

  return loss

def main_training_loop(V,U, epochs, pairs, learning_rate, word2idx, unigram_dic):
  #shuffle the pairs
  #loop over the epochs
  #update V and U
  losses = []

  for _ in range(epochs):
    random.shuffle(pairs)
    for pair in pairs:
      loss = training_step(pair, learning_rate, V, U, word2idx, unigram_dic)
      losses.append(loss)

  return losses

def word2vec(corpus, embedding_dim=50, epochs=50, learning_rate=0.01):
    tokens = tokenize(corpus)
    unique_words = unique_vocab(tokens)
    word2idx, _ = vocabulary_builder(tokens)
    word_freq = get_word_counts(tokens)
    word_pairs = create_skip_gram_pairs(tokens)
    unigram = build_unigram_distribution(word2idx, word_freq)
    V, U = initialize_embeddings(len(unique_words), embedding_dim)
    losses = main_training_loop(V, U, epochs, word_pairs, learning_rate, word2idx, unigram)
    return V, U, word2idx, losses

U,V, word2idx, losses = word2vec(corpus)

import matplotlib.pyplot as plt
print(f"Final loss: {losses[-1]:.4f}")
print(f"Initial loss: {losses[0]:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.title("Training Loss Over Time")
plt.xlabel("Epochs")
plt.ylabel("Average Loss per Epoch")
plt.grid(True)
plt.show()

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

def find_similar_words(word, word2idx, V, top_k=3):
    if word not in word2idx:
        return []

    word_idx = word2idx[word]
    word_vec = V[word_idx]

    similarities = []
    idx2word = {idx: word for word, idx in word2idx.items()}

    for idx in range(len(V)):
        if idx != word_idx:
            sim = cosine_similarity(word_vec, V[idx])
            similarities.append((idx2word[idx], sim))

    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

print("\nWord similarities:")
for word in ["cpu", "the", "arithmetic"]:
    if word in word2idx:
        similar = find_similar_words(word, word2idx, V)
        print(f"{word}: {similar}")

