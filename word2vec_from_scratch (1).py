# -*- coding: utf-8 -*-
"""word2vec_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLKKGTIDQ_AIw8tRlW4w9PzYXlXi9H5p
"""

import numpy as np
import collections
import random
import math
import re
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

small_corpus = """
 computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs, which enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster.

A broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users.

Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved."""

def create_comprehensive_gender_corpus():
    """
    Create a more comprehensive corpus with various gender relationships
    """

    comprehensive_corpus = """
    The king is a man who rules with authority. The queen is a woman who rules with wisdom.
    A man becomes king through royal lineage. A woman becomes queen through royal heritage.
    Every king is a man of great responsibility. Every queen is a woman of great influence.
    The man who is king protects his realm. The woman who is queen nurtures her domain.
    A brave man can become king. A wise woman can become queen.
    The king, being a man, leads his army into battle. The queen, being a woman, leads her court with grace.

    He is the king of this land. She is the queen of this realm.
    His majesty the king commands respect. Her majesty the queen inspires devotion.
    The male heir will become king. The female heir will become queen.
    A boy grows up to be a man and perhaps a king. A girl grows up to be a woman and perhaps a queen.

    The father was king before his son. The mother was queen before her daughter.
    A man's highest title is king. A woman's highest title is queen.
    The husband is king while the wife is queen.

    He rules as king over his subjects. She reigns as queen over her people.
    The man wore the crown of a king. The woman wore the crown of a queen.
    A king is a man with royal duties. A queen is a woman with royal privileges.

    The son will inherit and become king. The daughter will inherit and become queen.
    A man of royal blood becomes king. A woman of royal blood becomes queen.
    The prince will grow into a king. The princess will grow into a queen.

    In the palace lives a man who is king. In the castle lives a woman who is queen.
    The king is the most powerful man in the kingdom. The queen is the most powerful woman in the realm.

    A good man makes a good king. A good woman makes a good queen.
    The man on the throne is our king. The woman on the throne is our queen.
    Every king is someone's son, brother, or husband - a man first. Every queen is someone's daughter, sister, or wife - a woman first.

    The strong man became a mighty king. The intelligent woman became a wise queen.
    A man earns the title king through birthright or merit. A woman earns the title queen through birthright or marriage.

    Uncle is to aunt as king is to queen. Brother is to sister as king is to queen.
    Father is to mother as king is to queen. Son is to daughter as king is to queen.
    Husband is to wife as king is to queen. Man is to woman as king is to queen.

    The gentleman is a refined man. The lady is a refined woman.
    A lord is a man of nobility. A lady is a woman of nobility.
    The duke is a man of high rank. The duchess is a woman of high rank.

    Actor is to actress as king is to queen. Waiter is to waitress as man is to woman.
    Hero is to heroine as man is to woman. God is to goddess as king is to queen.

    He is masculine like a king. She is feminine like a queen.
    The man demonstrates strength. The woman demonstrates elegance.
    A king embodies male leadership. A queen embodies female leadership.

    The boy will become a man and maybe a king. The girl will become a woman and maybe a queen.
    A young man aspires to be king. A young woman aspires to be queen.
    The teenage boy dreams of being a powerful man like a king. The teenage girl dreams of being a powerful woman like a queen.

    History remembers great men who were kings. History remembers great women who were queens.
    The man who would be king must be worthy. The woman who would be queen must be deserving.
    A king represents the ideal man in his society. A queen represents the ideal woman in her society.
    """

    return comprehensive_corpus

def tokenize(corpus):
  sentences = re.split(r'[.!?]', corpus) # seperate into sentences
  sentences = [s.strip() for s in sentences if s.strip() != ""] #removes white space
  sentences = [re.sub(r'[^a-zA-Z\s\']', "", s ) for s in sentences] #remove the puncations
  tokens = [s.lower().split() for s in sentences] # tokenize
  return tokens

def unique_vocab(tokens):
  return list(set([word for token in tokens for word in token])) #flatten and create set to get unique words

def vocabulary_builder(tokens):
  unique = unique_vocab(tokens)
  word2idx = {word:i for i,word in enumerate(unique)} # word to index dictionary
  idx2word = {i:word for i,word in enumerate(unique)} # index to word dictionary
  return word2idx, idx2word

def get_word_counts(tokens):
  frequency = {}
  for sentences in tokens:
    for word in sentences:
      frequency[word] = frequency.get(word, 0) + 1
  return frequency

def create_skip_gram_pairs(tokens, window_size = 8): # create the training pairs with a default window size of 8
  pairs = []
  for sentence in tokens:
    sentence_len = len(sentence)
    for i, center_word in enumerate(sentence):
      for j in range(max(0, i - window_size), min(i + window_size + 1, sentence_len)):
        if i != j:
          pairs.append((center_word, sentence[j]))
  return pairs

def build_unigram_distribution(word2idx, word_counts):
  idx2prob = {}
  word_counts = {key: value**0.75 for key, value in word_counts.items()}
  total = sum(word_counts.values())
  unigram = {word2idx[key]: value / total for key, value in word_counts.items()}

  return unigram

def sample_negatives(word2idx,unigram_dic,pair, k=15):
  # since pair is a pair of words we have to turn them into the indicies
  exclude_idx = [word2idx[word] for word in pair]
  keys = []
  probs = []
  for key,value in unigram_dic.items():
    if key not in exclude_idx:
      keys.append(key)
      probs.append(value)

  # now we have to normalize again because now it doesn't add up to zero
  total = np.sum(probs)
  probs = [prob / total for prob in probs]

  negative_samples = []

  for i in range(k):
    rand = np.random.choice(np.arange(0, len(keys)), p=probs) #this is how we can get random numbers using a custom distribution
    negative_samples.append(keys[rand])

  return negative_samples

#so this is where we create the matrcies U and V for the word embeddings
# U corresponds to when the word is in the context
# V corresponds to when the word is in the center
# The order should be the same as the one in the word2idx, order
def initialize_embeddings(vocab_size, embedding_dim):
  low = (-1) / np.sqrt(embedding_dim)
  high = 1 / np.sqrt(embedding_dim)

  U = np.random.uniform(low, high, size = (vocab_size, embedding_dim)) #context
  V = np.random.uniform(low, high, size = (vocab_size, embedding_dim)) #center

  return V, U

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def compute_loss(v_c, u_o, u_k):
  sum = 0
  for u_neg in u_k:
    product = np.dot(v_c, u_neg)
    sig = sigmoid(-product)
    sum +=  np.log(sig)

  dot_prod = np.dot(u_o, v_c)
  sigmoid_of_prod = sigmoid(dot_prod)
  log_of_sig = np.log(sigmoid_of_prod)
  return (-log_of_sig - sum)

def compute_gradients(v_c, u_o, u_k):
    # Gradient w.r.t v_c
    sum_v_c = np.sum([sigmoid(np.dot(v_c, u_neg)) * u_neg for u_neg in u_k], axis=0)
    d_v_c = (sigmoid(np.dot(v_c, u_o)) - 1) * u_o + sum_v_c

    # Gradient w.r.t u_o
    d_u_o = (sigmoid(np.dot(u_o, v_c)) - 1) * v_c

    # Gradient w.r.t each u_k
    d_u_k = [sigmoid(np.dot(u_neg, v_c)) * v_c for u_neg in u_k]

    return d_v_c, d_u_o, d_u_k

#function for the training loop
"""
we first take in a pair from the skip-gram pairs, a learning rate, matrix U an V along with the word2idx dictionary
then we get the get the negative pairs, compute the dot product of the center vector and the context vector
get all the vectors for the negative samples.
compute the dot product of the venter vector and the outside vector
compute the dot for the negative samples with the center vector
get a positive score using sigmoid for the dot_product of the center and the real context
also the sigmoid of the negative samples and the center words
comput the loss
compute the gradients by computing the paritial derivative of loss w.r.t v_c, u_o and u_k
then we multiply these gradients by the learning rate and update U and V
and return the loss
"""

def training_step(pair, learning_rate, V, U, word2idx, unigram_dic):
  negative_samples = sample_negatives(word2idx, unigram_dic, pair) #these are the negative samples give us indicies
  center_idx = word2idx[pair[0]]
  outside_idx = word2idx[pair[1]]


  v_c = V[center_idx].copy()  #vector for the center word
  u_o = U[outside_idx].copy() #vector for the outside word
  u_k = [U[idx].copy() for idx in negative_samples]

  loss = compute_loss(v_c, u_o, u_k)

  d_v_c, d_u_o, d_u_k = compute_gradients(v_c, u_o, u_k)

  V[center_idx] -= learning_rate * d_v_c
  U[outside_idx] -= learning_rate * d_u_o

  for i,idx in enumerate(negative_samples):
    U[idx] -= learning_rate * d_u_k[i]

  return loss

def main_training_loop(V,U, epochs, pairs, learning_rate, word2idx, unigram_dic):
  #shuffle the pairs
  #loop over the epochs
  #update V and U
  losses = []

  for _ in range(epochs):
    random.shuffle(pairs)
    for pair in pairs:
      loss = training_step(pair, learning_rate, V, U, word2idx, unigram_dic)
      losses.append(loss)

  return losses

def word2vec(corpus, embedding_dim=100, epochs=100, learning_rate=0.01):
    tokens = tokenize(corpus)
    unique_words = unique_vocab(tokens)
    word2idx, _ = vocabulary_builder(tokens)
    word_freq = get_word_counts(tokens)
    word_pairs = create_skip_gram_pairs(tokens)
    unigram = build_unigram_distribution(word2idx, word_freq)
    V, U = initialize_embeddings(len(unique_words), embedding_dim)
    losses = main_training_loop(V, U, epochs, word_pairs, learning_rate, word2idx, unigram)
    return V, U, word2idx, losses

comprehensive_corpus = create_comprehensive_gender_corpus()

tokens = tokenize(comprehensive_corpus)
count = 0
for sentence in tokens:
  for word in sentence:
    count += 1
print(count)

U,V, word2idx, losses = word2vec(comprehensive_corpus)

final_embeddings = (U + V) / 2

words_to_plot = ["man", "king", "woman", "queen"]
indicies = [word2idx[word] for word in words_to_plot]
vectors_to_plot = np.array([final_embeddings[idx] for idx in indicies])
pca = PCA(n_components=2)
reduced = pca.fit_transform(vectors_to_plot)

plt.figure(figsize=(8,6))
for i,word in enumerate(words_to_plot):
  x, y = reduced[i]
  plt.scatter(x,y)
  plt.text(x + 0.02, y + 0.02, word, fontsize=12)

plt.title("Word Embeddings Visualized with PCA")
plt.grid(True)
plt.show()
plt.savefig("visualized")

print(f"Final loss: {losses[-1]:.4f}")
print(f"Initial loss: {losses[0]:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(losses)
plt.title("Training Loss Over Time")
plt.xlabel("Epochs")
plt.ylabel("Average Loss per Epoch")
window = 100
smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')
plt.plot(smoothed)
plt.savefig("loss_plot_main")

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

def word_analogy(word_a, word_b, word_c, word2idx, embeddings):
  # a is to b as c is to?

  for word in [word_a, word_b, word_c]:
    if word not in word2idx.keys():
      return f"word {word} not found in vocab"

  idx_a = word2idx[word_a]
  idx_b = word2idx[word_b]
  idx_c = word2idx[word_c]

  vec_a = embeddings[idx_a]
  vec_b = embeddings[idx_b]
  vec_c = embeddings[idx_c]

  analogy_vec = vec_b - vec_a + vec_c

  idx2word = {idx: word for word, idx in word2idx.items()}

  similarities = []

  for idx in range(len(embeddings)):
    if idx in [idx_a, idx_b, idx_c]:
      continue

    similarity = cosine_similarity(analogy_vec, embeddings[idx])
    similarities.append((idx2word[idx], similarity))

  similarities.sort(key =lambda x: x[1], reverse=True)

  return f"{word_a} is to {word_b} as {word_c} is to {similarities[0][0]}"

analogy = word_analogy("king", "man", "queen", word2idx, final_embeddings)
print(analogy)

